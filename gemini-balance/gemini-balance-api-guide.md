# Gemini Balance API è°ƒç”¨æŒ‡å¯¼æ–‡æ¡£

## ğŸ“– æ¦‚è¿°

Gemini Balance æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„ Gemini API ä»£ç†æœåŠ¡ï¼Œæä¾›å¤šç§æ ¼å¼çš„ API æ¥å£ï¼Œæ”¯æŒè´Ÿè½½å‡è¡¡ã€æµå¼å“åº”å’Œå¤šæ¨¡å‹è°ƒç”¨ã€‚æœ¬æ–‡æ¡£å°†è¯¦ç»†ä»‹ç»å¦‚ä½•è°ƒç”¨ Gemini Balance çš„å„ç§ APIã€‚

## ğŸ”§ æœåŠ¡é…ç½®

### åŸºæœ¬ä¿¡æ¯
- **æœåŠ¡åœ°å€**: `http://localhost:8000`
- **è®¤è¯Token**: `q1q2q3q4`
- **è®¤è¯æ–¹å¼**: Bearer Token
- **æ•°æ®åº“**: MySQL (root/Woaihujun123.)

### ç¯å¢ƒè¦æ±‚
- Docker å’Œ Docker Compose
- MySQL 5.7+ å®¹å™¨
- ç«¯å£ 8000 å¯ç”¨

## ğŸ”‘ è®¤è¯æ–¹å¼

æ‰€æœ‰ API è°ƒç”¨éƒ½éœ€è¦åœ¨è¯·æ±‚å¤´ä¸­åŒ…å«è®¤è¯ä¿¡æ¯ï¼š

```bash
Authorization: Bearer q1q2q3q4
```

## ğŸš€ API ç«¯ç‚¹è¯¦è§£

### 1. è·å–æ¨¡å‹åˆ—è¡¨

#### æ ‡å‡†æ ¼å¼
```bash
curl -H "Authorization: Bearer q1q2q3q4" \
     http://localhost:8000/v1/models
```

#### OpenAI å…¼å®¹æ ¼å¼
```bash
curl -H "Authorization: Bearer q1q2q3q4" \
     http://localhost:8000/openai/v1/models
```

#### HuggingFace å…¼å®¹æ ¼å¼
```bash
curl -H "Authorization: Bearer q1q2q3q4" \
     http://localhost:8000/hf/v1/models
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "object": "list",
  "data": [
    {
      "id": "gemini-1.5-flash",
      "object": "model",
      "created": 1752996930,
      "owned_by": "google"
    }
  ]
}
```

### 2. èŠå¤©å®Œæˆ API

#### åŸºç¡€èŠå¤©è¯·æ±‚
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer q1q2q3q4" \
  -d '{
    "model": "gemini-1.5-flash",
    "messages": [
      {
        "role": "user",
        "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
      }
    ],
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

#### æµå¼èŠå¤©è¯·æ±‚
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer q1q2q3q4" \
  -d '{
    "model": "gemini-1.5-flash",
    "messages": [
      {
        "role": "user", 
        "content": "ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"
      }
    ],
    "stream": true,
    "max_tokens": 150
  }'
```

#### å¤šè½®å¯¹è¯è¯·æ±‚
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer q1q2q3q4" \
  -d '{
    "model": "gemini-1.5-flash",
    "messages": [
      {
        "role": "user",
        "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
      },
      {
        "role": "assistant",
        "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯..."
      },
      {
        "role": "user",
        "content": "èƒ½ä¸¾ä¸ªå…·ä½“ä¾‹å­å—ï¼Ÿ"
      }
    ],
    "max_tokens": 200
  }'
```

### 3. OpenAI å…¼å®¹æ ¼å¼

```bash
curl -X POST http://localhost:8000/openai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer q1q2q3q4" \
  -d '{
    "model": "gemini-1.5-flash",
    "messages": [
      {
        "role": "user",
        "content": "Hello, how are you?"
      }
    ],
    "max_tokens": 50
  }'
```

### 4. æ–‡æœ¬åµŒå…¥ API

```bash
curl -X POST http://localhost:8000/v1/embeddings \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer q1q2q3q4" \
  -d '{
    "model": "text-embedding-004",
    "input": "è¿™æ˜¯ä¸€æ®µéœ€è¦ç”ŸæˆåµŒå…¥å‘é‡çš„æ–‡æœ¬"
  }'
```

### 5. å›¾åƒç”Ÿæˆ API

```bash
curl -X POST http://localhost:8000/v1/images/generations \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer q1q2q3q4" \
  -d '{
    "model": "imagen-3.0-generate-002",
    "prompt": "ä¸€åªå¯çˆ±çš„å°çŒ«åœ¨èŠ±å›­é‡Œç©è€",
    "n": 1,
    "size": "1024x1024"
  }'
```

## ğŸ¯ æ”¯æŒçš„æ¨¡å‹

### èŠå¤©æ¨¡å‹
- `gemini-1.5-pro-latest`
- `gemini-1.5-flash`
- `gemini-1.5-flash-8b`
- `gemini-2.5-pro`
- `gemini-2.5-flash`
- `gemini-2.0-flash-exp`
- `gemini-2.0-pro-exp`

### åµŒå…¥æ¨¡å‹
- `text-embedding-004`
- `embedding-001`
- `gemini-embedding-001`

### å›¾åƒç”Ÿæˆæ¨¡å‹
- `imagen-3.0-generate-002`
- `imagen-4.0-generate-preview-06-06`

## ğŸ“Š è¯·æ±‚å‚æ•°è¯´æ˜

### èŠå¤©å®Œæˆå‚æ•°
| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `model` | string | æ˜¯ | ä½¿ç”¨çš„æ¨¡å‹åç§° |
| `messages` | array | æ˜¯ | å¯¹è¯æ¶ˆæ¯æ•°ç»„ |
| `max_tokens` | integer | å¦ | æœ€å¤§ç”Ÿæˆtokenæ•° |
| `temperature` | float | å¦ | æ§åˆ¶éšæœºæ€§ (0.0-2.0) |
| `stream` | boolean | å¦ | æ˜¯å¦å¯ç”¨æµå¼å“åº” |
| `top_p` | float | å¦ | æ ¸é‡‡æ ·å‚æ•° |

### æ¶ˆæ¯æ ¼å¼
```json
{
  "role": "user|assistant|system",
  "content": "æ¶ˆæ¯å†…å®¹"
}
```

## ğŸŒ Web ç®¡ç†ç•Œé¢

è®¿é—® `http://localhost:8000` å¯ä»¥æ‰“å¼€ Web ç®¡ç†ç•Œé¢ï¼ŒåŠŸèƒ½åŒ…æ‹¬ï¼š

- æ¨¡å‹çŠ¶æ€ç›‘æ§
- API è°ƒç”¨ç»Ÿè®¡
- é…ç½®ç®¡ç†
- æ—¥å¿—æŸ¥çœ‹

ç™»å½•éœ€è¦ä½¿ç”¨è®¤è¯Token: `q1q2q3q4`

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„ç«¯ç‚¹æ ¼å¼
- **æ ‡å‡†æ ¼å¼** (`/v1/*`): æ¨èç”¨äºæ–°é¡¹ç›®ï¼Œæ€§èƒ½æœ€ä½³
- **OpenAIæ ¼å¼** (`/openai/v1/*`): é€‚åˆä»OpenAIè¿ç§»çš„é¡¹ç›®
- **HuggingFaceæ ¼å¼** (`/hf/v1/*`): é€‚åˆHuggingFaceç”Ÿæ€é›†æˆ

### 2. æ¨¡å‹é€‰æ‹©å»ºè®®
- **å¿«é€Ÿå“åº”**: `gemini-1.5-flash`, `gemini-1.5-flash-8b`
- **é«˜è´¨é‡è¾“å‡º**: `gemini-1.5-pro`, `gemini-2.5-pro`
- **å®éªŒæ€§åŠŸèƒ½**: `gemini-2.0-flash-exp`

### 3. æ€§èƒ½ä¼˜åŒ–
- ä½¿ç”¨æµå¼å“åº”æå‡ç”¨æˆ·ä½“éªŒ
- åˆç†è®¾ç½® `max_tokens` æ§åˆ¶å“åº”é•¿åº¦
- æ ¹æ®éœ€æ±‚è°ƒæ•´ `temperature` å‚æ•°

### 4. é”™è¯¯å¤„ç†
```bash
# æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
curl http://localhost:8000/health

# æŸ¥çœ‹APIç»Ÿè®¡ä¿¡æ¯
curl -H "Authorization: Bearer q1q2q3q4" \
     http://localhost:8000/stats
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **è®¤è¯å¤±è´¥ (401)**
   - æ£€æŸ¥Tokenæ˜¯å¦æ­£ç¡®: `q1q2q3q4`
   - ç¡®è®¤è¯·æ±‚å¤´æ ¼å¼: `Authorization: Bearer q1q2q3q4`

2. **æœåŠ¡ä¸å¯ç”¨ (503)**
   - æ£€æŸ¥æœåŠ¡çŠ¶æ€: `docker compose ps`
   - æŸ¥çœ‹æ—¥å¿—: `docker compose logs gemini-balance`

3. **æ•°æ®åº“è¿æ¥å¤±è´¥**
   - ç¡®è®¤MySQLå®¹å™¨è¿è¡Œæ­£å¸¸
   - éªŒè¯æ•°æ®åº“é…ç½®: root/Woaihujun123.

### æ—¥å¿—æŸ¥çœ‹
```bash
# æŸ¥çœ‹å®æ—¶æ—¥å¿—
docker compose logs -f gemini-balance

# æŸ¥çœ‹MySQLæ—¥å¿—
docker logs mysql5.7
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. æœåŠ¡è¿è¡ŒçŠ¶æ€
2. ç½‘ç»œè¿æ¥
3. è®¤è¯é…ç½®
4. æ¨¡å‹å¯ç”¨æ€§

## ğŸ’» ç¼–ç¨‹è¯­è¨€ç¤ºä¾‹

### Python ç¤ºä¾‹

```python
import requests
import json

# åŸºç¡€é…ç½®
BASE_URL = "http://localhost:8000"
API_KEY = "q1q2q3q4"
HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

# èŠå¤©å®Œæˆ
def chat_completion(message, model="gemini-1.5-flash"):
    url = f"{BASE_URL}/v1/chat/completions"
    data = {
        "model": model,
        "messages": [{"role": "user", "content": message}],
        "max_tokens": 150
    }

    response = requests.post(url, headers=HEADERS, json=data)
    return response.json()

# æµå¼èŠå¤©
def stream_chat(message, model="gemini-1.5-flash"):
    url = f"{BASE_URL}/v1/chat/completions"
    data = {
        "model": model,
        "messages": [{"role": "user", "content": message}],
        "stream": True,
        "max_tokens": 150
    }

    response = requests.post(url, headers=HEADERS, json=data, stream=True)
    for line in response.iter_lines():
        if line:
            print(line.decode('utf-8'))

# ä½¿ç”¨ç¤ºä¾‹
result = chat_completion("ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹è‡ªå·±")
print(json.dumps(result, indent=2, ensure_ascii=False))
```

### JavaScript/Node.js ç¤ºä¾‹

```javascript
const axios = require('axios');

const BASE_URL = 'http://localhost:8000';
const API_KEY = 'q1q2q3q4';

const headers = {
    'Authorization': `Bearer ${API_KEY}`,
    'Content-Type': 'application/json'
};

// èŠå¤©å®Œæˆ
async function chatCompletion(message, model = 'gemini-1.5-flash') {
    try {
        const response = await axios.post(`${BASE_URL}/v1/chat/completions`, {
            model: model,
            messages: [{ role: 'user', content: message }],
            max_tokens: 150
        }, { headers });

        return response.data;
    } catch (error) {
        console.error('Error:', error.response?.data || error.message);
    }
}

// è·å–æ¨¡å‹åˆ—è¡¨
async function getModels() {
    try {
        const response = await axios.get(`${BASE_URL}/v1/models`, { headers });
        return response.data;
    } catch (error) {
        console.error('Error:', error.response?.data || error.message);
    }
}

// ä½¿ç”¨ç¤ºä¾‹
chatCompletion('Hello, how are you?').then(result => {
    console.log(JSON.stringify(result, null, 2));
});
```

### PHP ç¤ºä¾‹

```php
<?php
$baseUrl = 'http://localhost:8000';
$apiKey = 'q1q2q3q4';

function chatCompletion($message, $model = 'gemini-1.5-flash') {
    global $baseUrl, $apiKey;

    $url = $baseUrl . '/v1/chat/completions';
    $data = [
        'model' => $model,
        'messages' => [
            ['role' => 'user', 'content' => $message]
        ],
        'max_tokens' => 150
    ];

    $options = [
        'http' => [
            'header' => [
                "Authorization: Bearer $apiKey",
                "Content-Type: application/json"
            ],
            'method' => 'POST',
            'content' => json_encode($data)
        ]
    ];

    $context = stream_context_create($options);
    $result = file_get_contents($url, false, $context);

    return json_decode($result, true);
}

// ä½¿ç”¨ç¤ºä¾‹
$result = chatCompletion('ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±');
echo json_encode($result, JSON_PRETTY_PRINT | JSON_UNESCAPED_UNICODE);
?>
```

## ğŸ”— é›†æˆç¤ºä¾‹

### ä¸ OpenAI SDK é›†æˆ

```python
# ä½¿ç”¨ OpenAI Python SDK
from openai import OpenAI

client = OpenAI(
    api_key="q1q2q3q4",
    base_url="http://localhost:8000/openai/v1"
)

response = client.chat.completions.create(
    model="gemini-1.5-flash",
    messages=[
        {"role": "user", "content": "Hello, world!"}
    ]
)

print(response.choices[0].message.content)
```

### ä¸ LangChain é›†æˆ

```python
from langchain.llms.base import LLM
from langchain.schema import BaseMessage
import requests

class GeminiBalanceLLM(LLM):
    def __init__(self, api_key="q1q2q3q4", base_url="http://localhost:8000"):
        self.api_key = api_key
        self.base_url = base_url

    def _call(self, prompt: str, stop=None) -> str:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        data = {
            "model": "gemini-1.5-flash",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 150
        }

        response = requests.post(
            f"{self.base_url}/v1/chat/completions",
            headers=headers,
            json=data
        )

        return response.json()["choices"][0]["message"]["content"]

    @property
    def _llm_type(self) -> str:
        return "gemini-balance"

# ä½¿ç”¨ç¤ºä¾‹
llm = GeminiBalanceLLM()
result = llm("è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½")
print(result)
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-07-20
**æœåŠ¡ç‰ˆæœ¬**: gemini-balance 2.2.0
